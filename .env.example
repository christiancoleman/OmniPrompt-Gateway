# LLM Helper Configuration
# Copy this file to .env and fill in your API keys

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
# Optional: Change the default model (gpt-4, gpt-4-turbo, gpt-3.5-turbo, etc.)
OPENAI_MODEL=gpt-4
# Optional: Custom endpoint for OpenAI-compatible APIs
# OPENAI_API_ENDPOINT=https://api.openai.com/v1/chat/completions

# Claude Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
# Optional: Change the Claude model
CLAUDE_MODEL=claude-3-sonnet-20240229
# Other options: claude-3-opus-20240229, claude-3-haiku-20240307

# Ollama Configuration (for local models)
# OLLAMA_ENDPOINT=http://localhost:11434/api/chat
# OLLAMA_MODEL=llama2

# LM Studio Configuration (OpenAI-compatible local server)
# LM Studio will be auto-detected if running on the default port
# LM_STUDIO_ENDPOINT=http://localhost:1234/v1/chat/completions
# LM_STUDIO_MODEL=local-model  # The model name loaded in LM Studio
# LM_STUDIO_SYSTEM_PROMPT=You are a helpful local AI assistant.
# LM_STUDIO_TEMPERATURE=0.7

# Generic system prompt (applies to all models unless overridden)
# GENERIC_SYSTEM_PROMPT=You are a helpful AI assistant specializing in software development.

# Optional: Custom system prompts for each model (overrides GENERIC_SYSTEM_PROMPT)
# GPT4_SYSTEM_PROMPT=You are a helpful coding assistant specializing in Python and Salesforce.
# GPT35_SYSTEM_PROMPT=You are a concise and direct assistant.
# CLAUDE_SYSTEM_PROMPT=You are Claude, a thoughtful and thorough assistant.
# OLLAMA_SYSTEM_PROMPT=You are a helpful local AI assistant.
# LM_STUDIO_SYSTEM_PROMPT=You are a powerful local AI running on LM Studio.

# Optional: Temperature settings (0.0 = deterministic, 1.0 = creative)
# GPT4_TEMPERATURE=0.7
# GPT35_TEMPERATURE=0.8
# CLAUDE_TEMPERATURE=0.7
# OLLAMA_TEMPERATURE=0.7
