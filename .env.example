# OmniPrompt Gateway Configuration
# Copy this file to .env and fill in your API keys

# ===== Provider Configuration =====
# Each provider has its own section with shared parameters

# ----- OpenAI Configuration -----
OPENAI_API_KEY=your-openai-api-key-here
# Models to enable (comma-separated list)
OPENAI_MODELS=o3,o1-pro,o1,o1-mini,o4-mini,gpt-3.5-turbo,gpt-4
# Provider settings
OPENAI_SYSTEM_PROMPT=You are a helpful AI assistant.
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096
# Optional: Custom endpoint for OpenAI-compatible APIs
# OPENAI_API_ENDPOINT=https://api.openai.com/v1/chat/completions

# ----- Anthropic (Claude) Configuration -----
ANTHROPIC_API_KEY=your-anthropic-api-key-here
# Models to enable (comma-separated list)
ANTHROPIC_MODELS=claude-opus-4-20250514,claude-sonnet-4-20250514,claude-3-7-sonnet-20250219,claude-3-opus-20240229
# Provider settings
ANTHROPIC_SYSTEM_PROMPT=You are Claude, a helpful AI assistant.
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=4096
# Optional: Custom endpoint
# ANTHROPIC_API_ENDPOINT=https://api.anthropic.com/v1/messages

# ----- Local LM Studio Configuration -----
# LM Studio will be auto-detected if running on the default port
# Models to enable (comma-separated list, use exact names from LM Studio)
LOCAL_LMSTUDIO_MODELS=deepseek-r1-distill-qwen-14b-abliterated-v2
# Provider settings
LOCAL_LMSTUDIO_SYSTEM_PROMPT=You are a helpful local AI assistant.
LOCAL_LMSTUDIO_TEMPERATURE=0.7
LOCAL_LMSTUDIO_MAX_TOKENS=4096
# Optional: Custom endpoint
# LM_STUDIO_ENDPOINT=http://localhost:1234/v1/chat/completions

# ----- Local Ollama Configuration -----
# Models to enable (comma-separated list)
LOCAL_OLLAMA_MODELS=deepseek-r1:14b
# Provider settings
LOCAL_OLLAMA_SYSTEM_PROMPT=You are a helpful local AI assistant.
LOCAL_OLLAMA_TEMPERATURE=0.7
LOCAL_OLLAMA_MAX_TOKENS=4096
# Note: Ollama uses 'num_predict' for max tokens internally
# Optional: Custom endpoint
# OLLAMA_ENDPOINT=http://localhost:11434/api/chat

# ===== Global Settings =====
# Generic system prompt (fallback if provider-specific not set)
GENERIC_SYSTEM_PROMPT=You are a helpful AI assistant.

# MCP (Model Context Protocol) Integration
ENABLE_MCP=true
# Directory that MCP filesystem server can access
MCP_FILESYSTEM_PATH=.

# ===== Network Settings =====
# Proxy Settings (if behind corporate firewall)
# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080

# SSL Certificate Bundle (for corporate environments)
# REQUESTS_CA_BUNDLE=C:/path/to/company-ca-bundle.crt
